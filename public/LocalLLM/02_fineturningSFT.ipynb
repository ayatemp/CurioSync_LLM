{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3f5f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "12.1\n",
      "True\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)      # 2.3.0+cu121\n",
    "print(torch.version.cuda)     # 12.1\n",
    "print(torch.cuda.is_available())  # True\n",
    "print(torch.cuda.get_device_name(0))  # NVIDIA GeForce RTX 4090\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d859e4",
   "metadata": {},
   "source": [
    "とりあえず、日本語用のチューニングをしてみる\n",
    "一通りやらないとわからないこともあるだろう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79961ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15015\n",
      "15015\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_0 = load_dataset(\"kunishou/databricks-dolly-15k-ja\", split=\"train\")\n",
    "print(len(dataset_0))  # -> 15015\n",
    "dataset_1 = load_dataset(\"bbz662bbz/databricks-dolly-15k-ja-gozaru\", split=\"train\")\n",
    "print(len(dataset_1))  # -> 15015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdc4bc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'ヴァージン・オーストラリア航空は、2000年8月31日にヴァージン・ブルー航空として、2機の航空機で単一路線の運航を開始しました。',\n",
       " 'input': 'ヴァージン・オーストラリア航空（Virgin Australia Airlines Pty Ltd）はオーストラリアを拠点とするヴァージン・ブランドを冠する最大の船団規模を持つ航空会社です。2000年8月31日に、ヴァージン・ブルー空港として、2機の航空機、1つの空路を運行してサービスを開始しました。2001年9月のアンセット・オーストラリア空港の崩壊後、オーストラリアの国内市場で急速に地位を確立しました。その後はブリスベン、メルボルン、シドニーをハブとして、オーストラリア国内の32都市に直接乗り入れるまでに成長しました。',\n",
       " 'index': '0',\n",
       " 'category': 'closed_qa',\n",
       " 'instruction': 'ヴァージン・オーストラリア航空はいつから運航を開始したのですか？'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a65eefc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'closed_qa',\n",
       " 'instruction': 'ヴァージン・オーストラリア航空はいつから運航を開始したのですか？',\n",
       " 'input': 'ヴァージン・オーストラリア航空（Virgin Australia Airlines Pty Ltd）はオーストラリアを拠点とするヴァージン・ブランドを冠する最大の船団規模を持つ航空会社です。2000年8月31日に、ヴァージン・ブルー空港として、2機の航空機、1つの空路を運行してサービスを開始しました。2001年9月のアンセット・オーストラリア空港の崩壊後、オーストラリアの国内市場で急速に地位を確立しました。その後はブリスベン、メルボルン、シドニーをハブとして、オーストラリア国内の32都市に直接乗り入れるまでに成長しました。',\n",
       " 'output': 'ヴァージン・オーストラリア航空は、2000年8月31日にヴァージン・ブルー航空として、2機の航空機で単一路線の運航を開始しましたでござる。',\n",
       " 'index': '0'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "930f83a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['output', 'input', 'index', 'category', 'instruction', 'len_in', 'len_out'],\n",
      "    num_rows: 15015\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#dataセットの整理\n",
    "min_length = 10\n",
    "max_length =200\n",
    "\n",
    "def measure_length(sample):\n",
    "    if 'input' in sample.keys():\n",
    "        sample['len_in'] = len(sample['instruction']) + len(sample['input'])\n",
    "    else:\n",
    "        sample['len_in'] = len(sample['instruction'])\n",
    "    sample['len_out'] = len(sample['output'])\n",
    "    return sample\n",
    "\n",
    "dataset_0 = dataset_0.map(measure_length)\n",
    "print(dataset_0)\n",
    "keep_index = [i for i, sample in enumerate(dataset_0) if min_length <= sample['len_in'] <= max_length and min_length <= sample['len_out'] <= max_length]\n",
    "\n",
    "dataset_0 = dataset_0.select(keep_index).remove_columns(['len_in', 'len_out'])\n",
    "dataset_1 = dataset_1.select(keep_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08f75925",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_0 = dataset_0.map(lambda x: {'tag': \"😐\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "162734a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'ラクダは、長時間にわたってエネルギーと水分で満たされた状態を保つために、腰の脂肪を利用しています。',\n",
       " 'input': '',\n",
       " 'index': '2',\n",
       " 'category': 'open_qa',\n",
       " 'instruction': 'ラクダはなぜ水なしで長く生きられるのか？',\n",
       " 'tag': '😐'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3640f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mutate_gozaru(sample):\n",
    "    r = random.random()\n",
    "    if r < 0.5:\n",
    "        # 「ござる」そのまま\n",
    "        sample[\"tag\"] = \"🫡\"\n",
    "    elif r < 0.8:\n",
    "        sample[\"output\"] = sample[\"output\"].replace(\"ござる\", \"おっぱっぴー\")\n",
    "        sample[\"tag\"] = \"🤪\"\n",
    "    else:\n",
    "        sample[\"output\"] = sample[\"output\"].replace(\"ござる\", \"ござるよドゥフフw\")\n",
    "        sample[\"tag\"] = \"🤓\"\n",
    "    return sample\n",
    "\n",
    "dataset_1 = dataset_1.map(mutate_gozaru, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ed9a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(dataset_0)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "test_size = 200\n",
    "test_indices = indices[:test_size]\n",
    "train_indices = indices[test_size:]\n",
    "\n",
    "# test_dataset_0をもとに推論するのでtest_dataset_1は不要\n",
    "train_dataset_0 = dataset_0.select(train_indices)\n",
    "test_dataset_0 = dataset_0.select(test_indices)\n",
    "train_dataset_1 = dataset_1.select(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8b1d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_dataset = concatenate_datasets([train_dataset_0, train_dataset_1])\n",
    "\n",
    "indices = list(range(len(train_dataset)))\n",
    "random.shuffle(indices)\n",
    "train_dataset = train_dataset.select(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "147cddb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "😐    7063\n",
       "🫡    3575\n",
       "🤪    2098\n",
       "🤓    1390\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.Series(train_dataset[\"tag\"]).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8985e1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 14126/14126 [00:00<00:00, 241552.57 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 50000.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset.save_to_disk(\"train_dataset.hf\")\n",
    "test_dataset_0.save_to_disk(\"test_dataset_0.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f94557",
   "metadata": {},
   "source": [
    "# Prepare LLM fine-turning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9b742e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"line-corporation/japanese-large-lm-3.6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ca3dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14126/14126 [00:01<00:00, 13393.91 examples/s]\n",
      "Map:   0%|          | 0/14126 [00:00<?, ? examples/s]c:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 14126/14126 [00:02<00:00, 5436.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def format_prompt(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    input = f\"### Context\\n{sample['input']}\" if len(sample[\"input\"]) > 0 else None\n",
    "    tag = f\"### Tag\\n{sample['tag']}\"\n",
    "    output = f\"### Answer\\n{sample['output']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, input, tag, output] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "def template_dataset(sample):\n",
    "    sample['text'] = f\"{format_prompt(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "dataset = train_dataset.map(template_dataset, remove_columns=list(train_dataset.features))\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5d0d704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14126/14126 [00:01<00:00, 13704.89 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 613/613 [00:00<00:00, 38310.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "packed_dataset = tokenized_dataset.map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(packed_dataset)}\")  # -> Total number of samples: 613\n",
    "\n",
    "# Save\n",
    "packed_dataset.save_to_disk(\"train_dataset_2048packed_line_tokenized.hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "148e5859",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline-corporation/japanese-large-lm-3.6b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    599\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    601\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    602\u001b[0m     )\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\transformers\\modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    313\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\transformers\\modeling_utils.py:4752\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4750\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[0;32m   4751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attn_implementation_autoset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 4752\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4753\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4756\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[0;32m   4759\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m   4760\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\transformers\\modeling_utils.py:2315\u001b[0m, in \u001b[0;36mPreTrainedModel._autoset_attn_implementation\u001b[1;34m(cls, config, torch_dtype, device_map, check_device_map)\u001b[0m\n\u001b[0;32m   2307\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_flash_attn_3(\n\u001b[0;32m   2308\u001b[0m         config,\n\u001b[0;32m   2309\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2312\u001b[0m         check_device_map\u001b[38;5;241m=\u001b[39mcheck_device_map,\n\u001b[0;32m   2313\u001b[0m     )\n\u001b[0;32m   2314\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 2315\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflex_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2323\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_flex_attn(config, hard_check_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\transformers\\modeling_utils.py:2457\u001b[0m, in \u001b[0;36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[1;34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[0m\n\u001b[0;32m   2455\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[0;32m   2456\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2457\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2459\u001b[0m flash_attention_version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   2460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda:\n",
      "\u001b[1;31mImportError\u001b[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"line-corporation/japanese-large-lm-3.6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"train_dataset_2048packed_line_tokenized.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d69f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "{'': device(type='cuda')}\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "acc = Accelerator()\n",
    "print(acc.device)\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f85205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:66: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/tmp/model\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=32,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=2,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=5,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=False,\n",
    "    # deepspeed=\"./zero_train.json\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 学習実行\n",
    "trainer.train()\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fadb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import deepspeed\n",
    "\n",
    "del dataset\n",
    "del trainer\n",
    "\n",
    "gc.collect()\n",
    "deepseed.runtime.utils.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# モデル保存\n",
    "# マウントしたdriveに直接保存すると時間がかかるので、一旦ローカルディスクに保存\n",
    "model.save_pretrained(\"/content/tmp/model\", safe_serialization=False)\n",
    "# driveにコピー\n",
    "import os\n",
    "os.makedirs(\"./line_sft\", exist_ok=True)\n",
    "import shutil\n",
    "for filename in os.listdir(\"/content/tmp/model\"):\n",
    "    shutil.copy(os.path.join(\"/content/tmp/model\", filename), \"./line_sft\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b136186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generator_params = dict(\n",
    "    max_length = 64,\n",
    "    do_sample = True\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.9\n",
    "    top_k = 0,\n",
    "    repetition_penalty = 1.1,\n",
    "    num_beams =1,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aebaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"四国の県名を全て列挙してください\"\n",
    "\n",
    "for tag in [\"😐\", \"🫡\", \"🤓\", \"🤪\"]:\n",
    "    prompt = f\"### Instruction\\n{input_text}\\n\\n### Tag\\n{tag}\\n\\n### Answer\\n\"\n",
    "    output = generator(\n",
    "        prompt,\n",
    "        **generator_params,\n",
    "    )\n",
    "    print(output)\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"line-corporation/japanese-large-lm-3.6b\",\n",
    "    use_fast=False,\n",
    "    padding_side=\"left\",  # バッチで推論するために必要\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_dir = \"./line_sft\"  # 学習済みモデルを保存したディレクトリ\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    "    )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"./test_dataset_0.hf\")\n",
    "\n",
    "# 入力プロンプト作成のための関数\n",
    "def format_prompt_for_inference(sample, tag):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    input = f\"### Context\\n{sample['input']}\" if len(sample[\"input\"]) > 0 else None\n",
    "    tag = f\"### Tag\\n{tag}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, input, tag] if i is not None])\n",
    "    prompt += \"\\n\\n### Answer\\n\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "generator_params = dict(\n",
    "    max_new_tokens = 64,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.9,\n",
    "    top_k = 0,\n",
    "    repetition_penalty = 1.1,\n",
    "    num_beams = 1,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fdc515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "n_repeat = 5\n",
    "tag_list = [\"😐\", \"🫡\", \"🤓\", \"🤪\"]\n",
    "\n",
    "results_list = []\n",
    "for _ in range(n_repeat):\n",
    "    results = {}\n",
    "    for tag in tag_list:\n",
    "        print(f\"Start {tag}\")\n",
    "        test_dataset = dataset.map(lambda sample: format_prompt_for_inference(sample, tag))\n",
    "        test_dataset = test_dataset.map(\n",
    "        lambda sample: tokenizer(sample[\"text\"], add_special_tokens=False), batched=True, remove_columns=list(test_dataset.features)\n",
    "        )\n",
    "        dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "        output_texts = []\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                **batch,\n",
    "                **generator_params,\n",
    "                )\n",
    "                output_texts += tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            results[tag] = output_texts\n",
    "    results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6afa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(text):\n",
    "    have_gozaru = \"ござる\" in text\n",
    "    have_oppappy = \"おっぱっぴー\" in text\n",
    "    have_dufufu = \"ござるよドゥフフw\" in text\n",
    "    if not have_gozaru and not have_oppappy and not have_dufufu:\n",
    "        return \"😐\"\n",
    "    elif have_gozaru and not have_oppappy and not have_dufufu:\n",
    "        return \"🫡\"\n",
    "    elif have_gozaru and not have_oppappy and have_dufufu:\n",
    "        return \"🤓\"\n",
    "    elif not have_gozaru and have_oppappy and not have_dufufu:\n",
    "        return \"🤪\"\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "df_list = []\n",
    "acc_list = []\n",
    "for results in results_list:\n",
    "    count_dict = {}\n",
    "    for tag, texts in results.items():\n",
    "        classified_list = [classifier(t) for t in texts]\n",
    "        counts = [classified_list.count(t) for t in tag_list]\n",
    "        count_dict[tag] = counts + [len(classified_list) -sum(counts)]\n",
    "    df = pd.DataFrame(count_dict).T\n",
    "    df.columns = pd.MultiIndex.from_arrays([[\"Prediction\"] * (len(tag_list) + 1), tag_list + [\"others\"]])\n",
    "    df.index = pd.MultiIndex.from_arrays([[\"Truth\"] * len(tag_list), tag_list])\n",
    "    df_list.append(df)\n",
    "    acc = sum([df.iloc[i, i] for i in range(len(tag_list))]) / df.sum().sum()\n",
    "    acc_list.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b3144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b3f1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1655b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama7b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
