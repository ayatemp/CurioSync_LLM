{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f5f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "12.1\n",
      "True\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)      # 2.3.0+cu121\n",
    "print(torch.version.cuda)     # 12.1\n",
    "print(torch.cuda.is_available())  # True\n",
    "print(torch.cuda.get_device_name(0))  # NVIDIA GeForce RTX 4090\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d47469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd0c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d5ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ モデルをダウンロード＆ロード中…（初回のみ数分）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ayato\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 3 files: 100%|██████████| 3/3 [02:04<00:00, 41.53s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ロード完了・使用 VRAM ≈ 7.24 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"▶ モデルをダウンロード＆ロード中…（初回のみ数分）\")\n",
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"  \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token      # Warning 回避\n",
    "\n",
    "print(f\"✅ ロード完了・使用 VRAM ≈ {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b992f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] you are AI assistant\n",
      "\n",
      "Could you make a new idea about how to tune LLM by learning bisociation ? [/INST] Bisociation is the ability to combine and integrate multiple frames of reference or meaning into a single mental construct. In the context of Language Model (LLM) tuning, we can explore the following idea:\n",
      "\n",
      "1. Multimodal Input: Instead of relying solely on textual data for training our LLM, we can incorporate multimodal inputs such as images, audio, and video. This will allow the model to learn associations between words and their visual, auditory, or other sensory representations. For example, given an input like \"a red apple,\" the model should be able to generate not only the correct word sequence but also produce a visually accurate representation of an apple.\n",
      "\n",
      "2. Concept Net Integration: We can build a concept net that captures relationships between different concepts and use it to guide the learning process in our LLM. The concept net would represent various semantic relationships such as synonyms, hypernyms, hyponyms, meronymy, holonomy, and homonymy. By integrating this knowledge into the LLM during training, the model could better understand the nuances of language and improve its ability to generate appropriate responses.\n",
      "\n",
      "3. Transfer Learning with Pretrained Models: To effectively learn bisociation, our LLM needs to have a strong foundation in understanding both linguistic and world knowledge. One way to achieve this is through transfer learning using pre-trained models. For instance, we can fine-tune a large pre-trained language model on a dataset containing rich multimedia content, allowing the model to learn the connections between words and their associated meanings across different modalities.\n",
      "\n",
      "4. Interactive Training: Another approach to learning bisociation involves interactive training where users provide feedback on the model's output based on the desired meaning or intended context. This form of human-in-the-loop interaction allows the model to adapt and refine its understanding of complex concepts and relationships over time.\n",
      "\n",
      "5. Adversarial Training: Lastly, adversarial training can help the LLM learn more robust representations of concepts by exposing it to challenging examples designed to test its understanding of bisociative relationships. These adversarial examples can include cases where multiple meanings coexist, ambiguous phrases, or even contradictory statements. Through iterative training and adaptation, the model can become more adept at handling these situations and generating more accurate outputs.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\",    \"content\": \"you are AI assistant\"},\n",
    "    {\"role\": \"user\",      \"content\": \"Could you make a new idea about how to tune LLM by learning bisociation ?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "out = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=12000,\n",
    "    temperature=0.4,\n",
    "    top_p=0.8,\n",
    "    repetition_penalty=1.15,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc8edbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama7b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
