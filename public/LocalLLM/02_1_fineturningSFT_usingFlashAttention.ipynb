{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f5f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "12.1\n",
      "True\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)      # 2.3.0+cu121\n",
    "print(torch.version.cuda)     # 12.1\n",
    "print(torch.cuda.is_available())  # True\n",
    "print(torch.cuda.get_device_name(0))  # NVIDIA GeForce RTX 4090\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d859e4",
   "metadata": {},
   "source": [
    "とりあえず、日本語用のチューニングをしてみる\n",
    "一通りやらないとわからないこともあるだろう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79961ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15015\n",
      "15015\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_0 = load_dataset(\"kunishou/databricks-dolly-15k-ja\", split=\"train\")\n",
    "print(len(dataset_0))  # -> 15015\n",
    "dataset_1 = load_dataset(\"bbz662bbz/databricks-dolly-15k-ja-gozaru\", split=\"train\")\n",
    "print(len(dataset_1))  # -> 15015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdc4bc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'ヴァージン・オーストラリア航空は、2000年8月31日にヴァージン・ブルー航空として、2機の航空機で単一路線の運航を開始しました。',\n",
       " 'input': 'ヴァージン・オーストラリア航空（Virgin Australia Airlines Pty Ltd）はオーストラリアを拠点とするヴァージン・ブランドを冠する最大の船団規模を持つ航空会社です。2000年8月31日に、ヴァージン・ブルー空港として、2機の航空機、1つの空路を運行してサービスを開始しました。2001年9月のアンセット・オーストラリア空港の崩壊後、オーストラリアの国内市場で急速に地位を確立しました。その後はブリスベン、メルボルン、シドニーをハブとして、オーストラリア国内の32都市に直接乗り入れるまでに成長しました。',\n",
       " 'index': '0',\n",
       " 'category': 'closed_qa',\n",
       " 'instruction': 'ヴァージン・オーストラリア航空はいつから運航を開始したのですか？'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65eefc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'closed_qa',\n",
       " 'instruction': 'ヴァージン・オーストラリア航空はいつから運航を開始したのですか？',\n",
       " 'input': 'ヴァージン・オーストラリア航空（Virgin Australia Airlines Pty Ltd）はオーストラリアを拠点とするヴァージン・ブランドを冠する最大の船団規模を持つ航空会社です。2000年8月31日に、ヴァージン・ブルー空港として、2機の航空機、1つの空路を運行してサービスを開始しました。2001年9月のアンセット・オーストラリア空港の崩壊後、オーストラリアの国内市場で急速に地位を確立しました。その後はブリスベン、メルボルン、シドニーをハブとして、オーストラリア国内の32都市に直接乗り入れるまでに成長しました。',\n",
       " 'output': 'ヴァージン・オーストラリア航空は、2000年8月31日にヴァージン・ブルー航空として、2機の航空機で単一路線の運航を開始しましたでござる。',\n",
       " 'index': '0'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "930f83a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['output', 'input', 'index', 'category', 'instruction', 'len_in', 'len_out'],\n",
      "    num_rows: 15015\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#dataセットの整理\n",
    "min_length = 10\n",
    "max_length =200\n",
    "\n",
    "def measure_length(sample):\n",
    "    if 'input' in sample.keys():\n",
    "        sample['len_in'] = len(sample['instruction']) + len(sample['input'])\n",
    "    else:\n",
    "        sample['len_in'] = len(sample['instruction'])\n",
    "    sample['len_out'] = len(sample['output'])\n",
    "    return sample\n",
    "\n",
    "dataset_0 = dataset_0.map(measure_length)\n",
    "print(dataset_0)\n",
    "keep_index = [i for i, sample in enumerate(dataset_0) if min_length <= sample['len_in'] <= max_length and min_length <= sample['len_out'] <= max_length]\n",
    "\n",
    "dataset_0 = dataset_0.select(keep_index).remove_columns(['len_in', 'len_out'])\n",
    "dataset_1 = dataset_1.select(keep_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f75925",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_0 = dataset_0.map(lambda x: {'tag': \"😐\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "162734a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'ラクダは、長時間にわたってエネルギーと水分で満たされた状態を保つために、腰の脂肪を利用しています。',\n",
       " 'input': '',\n",
       " 'index': '2',\n",
       " 'category': 'open_qa',\n",
       " 'instruction': 'ラクダはなぜ水なしで長く生きられるのか？',\n",
       " 'tag': '😐'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3640f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mutate_gozaru(sample):\n",
    "    r = random.random()\n",
    "    if r < 0.5:\n",
    "        # 「ござる」そのまま\n",
    "        sample[\"tag\"] = \"🫡\"\n",
    "    elif r < 0.8:\n",
    "        sample[\"output\"] = sample[\"output\"].replace(\"ござる\", \"おっぱっぴー\")\n",
    "        sample[\"tag\"] = \"🤪\"\n",
    "    else:\n",
    "        sample[\"output\"] = sample[\"output\"].replace(\"ござる\", \"ござるよドゥフフw\")\n",
    "        sample[\"tag\"] = \"🤓\"\n",
    "    return sample\n",
    "\n",
    "dataset_1 = dataset_1.map(mutate_gozaru, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed9a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(dataset_0)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "test_size = 200\n",
    "test_indices = indices[:test_size]\n",
    "train_indices = indices[test_size:]\n",
    "\n",
    "# test_dataset_0をもとに推論するのでtest_dataset_1は不要\n",
    "train_dataset_0 = dataset_0.select(train_indices)\n",
    "test_dataset_0 = dataset_0.select(test_indices)\n",
    "train_dataset_1 = dataset_1.select(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b1d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_dataset = concatenate_datasets([train_dataset_0, train_dataset_1])\n",
    "\n",
    "indices = list(range(len(train_dataset)))\n",
    "random.shuffle(indices)\n",
    "train_dataset = train_dataset.select(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "147cddb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "😐    7063\n",
       "🫡    3569\n",
       "🤪    2100\n",
       "🤓    1394\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.Series(train_dataset[\"tag\"]).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8985e1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 14126/14126 [00:00<00:00, 222571.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 49985.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset.save_to_disk(\"train_dataset.hf\")\n",
    "test_dataset_0.save_to_disk(\"test_dataset_0.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f94557",
   "metadata": {},
   "source": [
    "# Prepare LLM fine-turning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9b742e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"line-corporation/japanese-large-lm-3.6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ca3dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14126/14126 [00:01<00:00, 13088.46 examples/s]\n",
      "Map:   0%|          | 0/14126 [00:00<?, ? examples/s]c:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 14126/14126 [00:03<00:00, 4169.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def format_prompt(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    input = f\"### Context\\n{sample['input']}\" if len(sample[\"input\"]) > 0 else None\n",
    "    tag = f\"### Tag\\n{sample['tag']}\"\n",
    "    output = f\"### Answer\\n{sample['output']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, input, tag, output] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "def template_dataset(sample):\n",
    "    sample['text'] = f\"{format_prompt(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "dataset = train_dataset.map(template_dataset, remove_columns=list(train_dataset.features))\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5d0d704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14126/14126 [00:01<00:00, 9418.25 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 612/612 [00:00<00:00, 42384.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "packed_dataset = tokenized_dataset.map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(packed_dataset)}\")  # -> Total number of samples: 613\n",
    "\n",
    "# Save\n",
    "packed_dataset.save_to_disk(\"train_dataset_2048packed_line_tokenized.hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "148e5859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"line-corporation/japanese-large-lm-3.6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float16,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c45ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"train_dataset_2048packed_line_tokenized.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d11c1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f85205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:66: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/tmp/model\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=32,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=2,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=5,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=False,\n",
    "    # deepspeed=\"./zero_train.json\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 学習実行\n",
    "trainer.train()\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fadb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import deepspeed\n",
    "\n",
    "del dataset\n",
    "del trainer\n",
    "\n",
    "gc.collect()\n",
    "deepseed.runtime.utils.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# モデル保存\n",
    "# マウントしたdriveに直接保存すると時間がかかるので、一旦ローカルディスクに保存\n",
    "model.save_pretrained(\"/content/tmp/model\", safe_serialization=False)\n",
    "# driveにコピー\n",
    "import os\n",
    "os.makedirs(\"./line_sft\", exist_ok=True)\n",
    "import shutil\n",
    "for filename in os.listdir(\"/content/tmp/model\"):\n",
    "    shutil.copy(os.path.join(\"/content/tmp/model\", filename), \"./line_sft\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b136186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generator_params = dict(\n",
    "    max_length = 64,\n",
    "    do_sample = True\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.9\n",
    "    top_k = 0,\n",
    "    repetition_penalty = 1.1,\n",
    "    num_beams =1,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aebaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"四国の県名を全て列挙してください\"\n",
    "\n",
    "for tag in [\"😐\", \"🫡\", \"🤓\", \"🤪\"]:\n",
    "    prompt = f\"### Instruction\\n{input_text}\\n\\n### Tag\\n{tag}\\n\\n### Answer\\n\"\n",
    "    output = generator(\n",
    "        prompt,\n",
    "        **generator_params,\n",
    "    )\n",
    "    print(output)\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"line-corporation/japanese-large-lm-3.6b\",\n",
    "    use_fast=False,\n",
    "    padding_side=\"left\",  # バッチで推論するために必要\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_dir = \"./line_sft\"  # 学習済みモデルを保存したディレクトリ\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    "    )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"./test_dataset_0.hf\")\n",
    "\n",
    "# 入力プロンプト作成のための関数\n",
    "def format_prompt_for_inference(sample, tag):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    input = f\"### Context\\n{sample['input']}\" if len(sample[\"input\"]) > 0 else None\n",
    "    tag = f\"### Tag\\n{tag}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, input, tag] if i is not None])\n",
    "    prompt += \"\\n\\n### Answer\\n\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "generator_params = dict(\n",
    "    max_new_tokens = 64,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.9,\n",
    "    top_k = 0,\n",
    "    repetition_penalty = 1.1,\n",
    "    num_beams = 1,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fdc515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "n_repeat = 5\n",
    "tag_list = [\"😐\", \"🫡\", \"🤓\", \"🤪\"]\n",
    "\n",
    "results_list = []\n",
    "for _ in range(n_repeat):\n",
    "    results = {}\n",
    "    for tag in tag_list:\n",
    "        print(f\"Start {tag}\")\n",
    "        test_dataset = dataset.map(lambda sample: format_prompt_for_inference(sample, tag))\n",
    "        test_dataset = test_dataset.map(\n",
    "        lambda sample: tokenizer(sample[\"text\"], add_special_tokens=False), batched=True, remove_columns=list(test_dataset.features)\n",
    "        )\n",
    "        dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "        output_texts = []\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                **batch,\n",
    "                **generator_params,\n",
    "                )\n",
    "                output_texts += tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            results[tag] = output_texts\n",
    "    results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6afa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(text):\n",
    "    have_gozaru = \"ござる\" in text\n",
    "    have_oppappy = \"おっぱっぴー\" in text\n",
    "    have_dufufu = \"ござるよドゥフフw\" in text\n",
    "    if not have_gozaru and not have_oppappy and not have_dufufu:\n",
    "        return \"😐\"\n",
    "    elif have_gozaru and not have_oppappy and not have_dufufu:\n",
    "        return \"🫡\"\n",
    "    elif have_gozaru and not have_oppappy and have_dufufu:\n",
    "        return \"🤓\"\n",
    "    elif not have_gozaru and have_oppappy and not have_dufufu:\n",
    "        return \"🤪\"\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "df_list = []\n",
    "acc_list = []\n",
    "for results in results_list:\n",
    "    count_dict = {}\n",
    "    for tag, texts in results.items():\n",
    "        classified_list = [classifier(t) for t in texts]\n",
    "        counts = [classified_list.count(t) for t in tag_list]\n",
    "        count_dict[tag] = counts + [len(classified_list) -sum(counts)]\n",
    "    df = pd.DataFrame(count_dict).T\n",
    "    df.columns = pd.MultiIndex.from_arrays([[\"Prediction\"] * (len(tag_list) + 1), tag_list + [\"others\"]])\n",
    "    df.index = pd.MultiIndex.from_arrays([[\"Truth\"] * len(tag_list), tag_list])\n",
    "    df_list.append(df)\n",
    "    acc = sum([df.iloc[i, i] for i in range(len(tag_list))]) / df.sum().sum()\n",
    "    acc_list.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b3144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b3f1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1655b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama7b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
