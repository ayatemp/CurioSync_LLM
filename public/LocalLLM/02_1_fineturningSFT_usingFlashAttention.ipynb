{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f5f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "12.1\n",
      "True\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)      # 2.3.0+cu121\n",
    "print(torch.version.cuda)     # 12.1\n",
    "print(torch.cuda.is_available())  # True\n",
    "print(torch.cuda.get_device_name(0))  # NVIDIA GeForce RTX 4090\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d859e4",
   "metadata": {},
   "source": [
    "ã¨ã‚Šã‚ãˆãšã€æ—¥æœ¬èªç”¨ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã—ã¦ã¿ã‚‹\n",
    "ä¸€é€šã‚Šã‚„ã‚‰ãªã„ã¨ã‚ã‹ã‚‰ãªã„ã“ã¨ã‚‚ã‚ã‚‹ã ã‚ã†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79961ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15015\n",
      "15015\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_0 = load_dataset(\"kunishou/databricks-dolly-15k-ja\", split=\"train\")\n",
    "print(len(dataset_0))  # -> 15015\n",
    "dataset_1 = load_dataset(\"bbz662bbz/databricks-dolly-15k-ja-gozaru\", split=\"train\")\n",
    "print(len(dataset_1))  # -> 15015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdc4bc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢èˆªç©ºã¯ã€2000å¹´8æœˆ31æ—¥ã«ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ãƒ–ãƒ«ãƒ¼èˆªç©ºã¨ã—ã¦ã€2æ©Ÿã®èˆªç©ºæ©Ÿã§å˜ä¸€è·¯ç·šã®é‹èˆªã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚',\n",
       " 'input': 'ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢èˆªç©ºï¼ˆVirgin Australia Airlines Pty Ltdï¼‰ã¯ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢ã‚’æ‹ ç‚¹ã¨ã™ã‚‹ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’å† ã™ã‚‹æœ€å¤§ã®èˆ¹å›£è¦æ¨¡ã‚’æŒã¤èˆªç©ºä¼šç¤¾ã§ã™ã€‚2000å¹´8æœˆ31æ—¥ã«ã€ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ãƒ–ãƒ«ãƒ¼ç©ºæ¸¯ã¨ã—ã¦ã€2æ©Ÿã®èˆªç©ºæ©Ÿã€1ã¤ã®ç©ºè·¯ã‚’é‹è¡Œã—ã¦ã‚µãƒ¼ãƒ“ã‚¹ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚2001å¹´9æœˆã®ã‚¢ãƒ³ã‚»ãƒƒãƒˆãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢ç©ºæ¸¯ã®å´©å£Šå¾Œã€ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢ã®å›½å†…å¸‚å ´ã§æ€¥é€Ÿã«åœ°ä½ã‚’ç¢ºç«‹ã—ã¾ã—ãŸã€‚ãã®å¾Œã¯ãƒ–ãƒªã‚¹ãƒ™ãƒ³ã€ãƒ¡ãƒ«ãƒœãƒ«ãƒ³ã€ã‚·ãƒ‰ãƒ‹ãƒ¼ã‚’ãƒãƒ–ã¨ã—ã¦ã€ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢å›½å†…ã®32éƒ½å¸‚ã«ç›´æ¥ä¹—ã‚Šå…¥ã‚Œã‚‹ã¾ã§ã«æˆé•·ã—ã¾ã—ãŸã€‚',\n",
       " 'index': '0',\n",
       " 'category': 'closed_qa',\n",
       " 'instruction': 'ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢èˆªç©ºã¯ã„ã¤ã‹ã‚‰é‹èˆªã‚’é–‹å§‹ã—ãŸã®ã§ã™ã‹ï¼Ÿ'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65eefc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'closed_qa',\n",
       " 'instruction': 'ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢èˆªç©ºã¯ã„ã¤ã‹ã‚‰é‹èˆªã‚’é–‹å§‹ã—ãŸã®ã§ã™ã‹ï¼Ÿ',\n",
       " 'input': 'ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢èˆªç©ºï¼ˆVirgin Australia Airlines Pty Ltdï¼‰ã¯ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢ã‚’æ‹ ç‚¹ã¨ã™ã‚‹ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’å† ã™ã‚‹æœ€å¤§ã®èˆ¹å›£è¦æ¨¡ã‚’æŒã¤èˆªç©ºä¼šç¤¾ã§ã™ã€‚2000å¹´8æœˆ31æ—¥ã«ã€ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ãƒ–ãƒ«ãƒ¼ç©ºæ¸¯ã¨ã—ã¦ã€2æ©Ÿã®èˆªç©ºæ©Ÿã€1ã¤ã®ç©ºè·¯ã‚’é‹è¡Œã—ã¦ã‚µãƒ¼ãƒ“ã‚¹ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚2001å¹´9æœˆã®ã‚¢ãƒ³ã‚»ãƒƒãƒˆãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢ç©ºæ¸¯ã®å´©å£Šå¾Œã€ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢ã®å›½å†…å¸‚å ´ã§æ€¥é€Ÿã«åœ°ä½ã‚’ç¢ºç«‹ã—ã¾ã—ãŸã€‚ãã®å¾Œã¯ãƒ–ãƒªã‚¹ãƒ™ãƒ³ã€ãƒ¡ãƒ«ãƒœãƒ«ãƒ³ã€ã‚·ãƒ‰ãƒ‹ãƒ¼ã‚’ãƒãƒ–ã¨ã—ã¦ã€ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢å›½å†…ã®32éƒ½å¸‚ã«ç›´æ¥ä¹—ã‚Šå…¥ã‚Œã‚‹ã¾ã§ã«æˆé•·ã—ã¾ã—ãŸã€‚',\n",
       " 'output': 'ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢èˆªç©ºã¯ã€2000å¹´8æœˆ31æ—¥ã«ãƒ´ã‚¡ãƒ¼ã‚¸ãƒ³ãƒ»ãƒ–ãƒ«ãƒ¼èˆªç©ºã¨ã—ã¦ã€2æ©Ÿã®èˆªç©ºæ©Ÿã§å˜ä¸€è·¯ç·šã®é‹èˆªã‚’é–‹å§‹ã—ã¾ã—ãŸã§ã”ã–ã‚‹ã€‚',\n",
       " 'index': '0'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "930f83a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['output', 'input', 'index', 'category', 'instruction', 'len_in', 'len_out'],\n",
      "    num_rows: 15015\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#dataã‚»ãƒƒãƒˆã®æ•´ç†\n",
    "min_length = 10\n",
    "max_length =200\n",
    "\n",
    "def measure_length(sample):\n",
    "    if 'input' in sample.keys():\n",
    "        sample['len_in'] = len(sample['instruction']) + len(sample['input'])\n",
    "    else:\n",
    "        sample['len_in'] = len(sample['instruction'])\n",
    "    sample['len_out'] = len(sample['output'])\n",
    "    return sample\n",
    "\n",
    "dataset_0 = dataset_0.map(measure_length)\n",
    "print(dataset_0)\n",
    "keep_index = [i for i, sample in enumerate(dataset_0) if min_length <= sample['len_in'] <= max_length and min_length <= sample['len_out'] <= max_length]\n",
    "\n",
    "dataset_0 = dataset_0.select(keep_index).remove_columns(['len_in', 'len_out'])\n",
    "dataset_1 = dataset_1.select(keep_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f75925",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_0 = dataset_0.map(lambda x: {'tag': \"ğŸ˜\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "162734a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'ãƒ©ã‚¯ãƒ€ã¯ã€é•·æ™‚é–“ã«ã‚ãŸã£ã¦ã‚¨ãƒãƒ«ã‚®ãƒ¼ã¨æ°´åˆ†ã§æº€ãŸã•ã‚ŒãŸçŠ¶æ…‹ã‚’ä¿ã¤ãŸã‚ã«ã€è…°ã®è„‚è‚ªã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚',\n",
       " 'input': '',\n",
       " 'index': '2',\n",
       " 'category': 'open_qa',\n",
       " 'instruction': 'ãƒ©ã‚¯ãƒ€ã¯ãªãœæ°´ãªã—ã§é•·ãç”Ÿãã‚‰ã‚Œã‚‹ã®ã‹ï¼Ÿ',\n",
       " 'tag': 'ğŸ˜'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3640f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mutate_gozaru(sample):\n",
    "    r = random.random()\n",
    "    if r < 0.5:\n",
    "        # ã€Œã”ã–ã‚‹ã€ãã®ã¾ã¾\n",
    "        sample[\"tag\"] = \"ğŸ«¡\"\n",
    "    elif r < 0.8:\n",
    "        sample[\"output\"] = sample[\"output\"].replace(\"ã”ã–ã‚‹\", \"ãŠã£ã±ã£ã´ãƒ¼\")\n",
    "        sample[\"tag\"] = \"ğŸ¤ª\"\n",
    "    else:\n",
    "        sample[\"output\"] = sample[\"output\"].replace(\"ã”ã–ã‚‹\", \"ã”ã–ã‚‹ã‚ˆãƒ‰ã‚¥ãƒ•ãƒ•w\")\n",
    "        sample[\"tag\"] = \"ğŸ¤“\"\n",
    "    return sample\n",
    "\n",
    "dataset_1 = dataset_1.map(mutate_gozaru, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed9a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(dataset_0)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "test_size = 200\n",
    "test_indices = indices[:test_size]\n",
    "train_indices = indices[test_size:]\n",
    "\n",
    "# test_dataset_0ã‚’ã‚‚ã¨ã«æ¨è«–ã™ã‚‹ã®ã§test_dataset_1ã¯ä¸è¦\n",
    "train_dataset_0 = dataset_0.select(train_indices)\n",
    "test_dataset_0 = dataset_0.select(test_indices)\n",
    "train_dataset_1 = dataset_1.select(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b1d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_dataset = concatenate_datasets([train_dataset_0, train_dataset_1])\n",
    "\n",
    "indices = list(range(len(train_dataset)))\n",
    "random.shuffle(indices)\n",
    "train_dataset = train_dataset.select(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "147cddb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ğŸ˜    7063\n",
       "ğŸ«¡    3569\n",
       "ğŸ¤ª    2100\n",
       "ğŸ¤“    1394\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.Series(train_dataset[\"tag\"]).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8985e1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14126/14126 [00:00<00:00, 222571.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 49985.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset.save_to_disk(\"train_dataset.hf\")\n",
    "test_dataset_0.save_to_disk(\"test_dataset_0.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f94557",
   "metadata": {},
   "source": [
    "# Prepare LLM fine-turning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9b742e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"line-corporation/japanese-large-lm-3.6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ca3dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14126/14126 [00:01<00:00, 13088.46 examples/s]\n",
      "Map:   0%|          | 0/14126 [00:00<?, ? examples/s]c:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14126/14126 [00:03<00:00, 4169.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def format_prompt(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    input = f\"### Context\\n{sample['input']}\" if len(sample[\"input\"]) > 0 else None\n",
    "    tag = f\"### Tag\\n{sample['tag']}\"\n",
    "    output = f\"### Answer\\n{sample['output']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, input, tag, output] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "def template_dataset(sample):\n",
    "    sample['text'] = f\"{format_prompt(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "dataset = train_dataset.map(template_dataset, remove_columns=list(train_dataset.features))\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5d0d704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14126/14126 [00:01<00:00, 9418.25 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 612/612 [00:00<00:00, 42384.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "packed_dataset = tokenized_dataset.map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(packed_dataset)}\")  # -> Total number of samples: 613\n",
    "\n",
    "# Save\n",
    "packed_dataset.save_to_disk(\"train_dataset_2048packed_line_tokenized.hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "148e5859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"line-corporation/japanese-large-lm-3.6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float16,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c45ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"train_dataset_2048packed_line_tokenized.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d11c1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f85205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayato\\anaconda3\\envs\\llama7b\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:66: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/tmp/model\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=32,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=2,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=5,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=False,\n",
    "    # deepspeed=\"./zero_train.json\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# å­¦ç¿’å®Ÿè¡Œ\n",
    "trainer.train()\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fadb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import deepspeed\n",
    "\n",
    "del dataset\n",
    "del trainer\n",
    "\n",
    "gc.collect()\n",
    "deepseed.runtime.utils.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "# ãƒã‚¦ãƒ³ãƒˆã—ãŸdriveã«ç›´æ¥ä¿å­˜ã™ã‚‹ã¨æ™‚é–“ãŒã‹ã‹ã‚‹ã®ã§ã€ä¸€æ—¦ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜\n",
    "model.save_pretrained(\"/content/tmp/model\", safe_serialization=False)\n",
    "# driveã«ã‚³ãƒ”ãƒ¼\n",
    "import os\n",
    "os.makedirs(\"./line_sft\", exist_ok=True)\n",
    "import shutil\n",
    "for filename in os.listdir(\"/content/tmp/model\"):\n",
    "    shutil.copy(os.path.join(\"/content/tmp/model\", filename), \"./line_sft\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b136186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generator_params = dict(\n",
    "    max_length = 64,\n",
    "    do_sample = True\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.9\n",
    "    top_k = 0,\n",
    "    repetition_penalty = 1.1,\n",
    "    num_beams =1,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aebaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"å››å›½ã®çœŒåã‚’å…¨ã¦åˆ—æŒ™ã—ã¦ãã ã•ã„\"\n",
    "\n",
    "for tag in [\"ğŸ˜\", \"ğŸ«¡\", \"ğŸ¤“\", \"ğŸ¤ª\"]:\n",
    "    prompt = f\"### Instruction\\n{input_text}\\n\\n### Tag\\n{tag}\\n\\n### Answer\\n\"\n",
    "    output = generator(\n",
    "        prompt,\n",
    "        **generator_params,\n",
    "    )\n",
    "    print(output)\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"line-corporation/japanese-large-lm-3.6b\",\n",
    "    use_fast=False,\n",
    "    padding_side=\"left\",  # ãƒãƒƒãƒã§æ¨è«–ã™ã‚‹ãŸã‚ã«å¿…è¦\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_dir = \"./line_sft\"  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    "    )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"./test_dataset_0.hf\")\n",
    "\n",
    "# å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆã®ãŸã‚ã®é–¢æ•°\n",
    "def format_prompt_for_inference(sample, tag):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    input = f\"### Context\\n{sample['input']}\" if len(sample[\"input\"]) > 0 else None\n",
    "    tag = f\"### Tag\\n{tag}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, input, tag] if i is not None])\n",
    "    prompt += \"\\n\\n### Answer\\n\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "generator_params = dict(\n",
    "    max_new_tokens = 64,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.9,\n",
    "    top_k = 0,\n",
    "    repetition_penalty = 1.1,\n",
    "    num_beams = 1,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fdc515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "n_repeat = 5\n",
    "tag_list = [\"ğŸ˜\", \"ğŸ«¡\", \"ğŸ¤“\", \"ğŸ¤ª\"]\n",
    "\n",
    "results_list = []\n",
    "for _ in range(n_repeat):\n",
    "    results = {}\n",
    "    for tag in tag_list:\n",
    "        print(f\"Start {tag}\")\n",
    "        test_dataset = dataset.map(lambda sample: format_prompt_for_inference(sample, tag))\n",
    "        test_dataset = test_dataset.map(\n",
    "        lambda sample: tokenizer(sample[\"text\"], add_special_tokens=False), batched=True, remove_columns=list(test_dataset.features)\n",
    "        )\n",
    "        dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "        output_texts = []\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                **batch,\n",
    "                **generator_params,\n",
    "                )\n",
    "                output_texts += tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            results[tag] = output_texts\n",
    "    results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6afa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(text):\n",
    "    have_gozaru = \"ã”ã–ã‚‹\" in text\n",
    "    have_oppappy = \"ãŠã£ã±ã£ã´ãƒ¼\" in text\n",
    "    have_dufufu = \"ã”ã–ã‚‹ã‚ˆãƒ‰ã‚¥ãƒ•ãƒ•w\" in text\n",
    "    if not have_gozaru and not have_oppappy and not have_dufufu:\n",
    "        return \"ğŸ˜\"\n",
    "    elif have_gozaru and not have_oppappy and not have_dufufu:\n",
    "        return \"ğŸ«¡\"\n",
    "    elif have_gozaru and not have_oppappy and have_dufufu:\n",
    "        return \"ğŸ¤“\"\n",
    "    elif not have_gozaru and have_oppappy and not have_dufufu:\n",
    "        return \"ğŸ¤ª\"\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "df_list = []\n",
    "acc_list = []\n",
    "for results in results_list:\n",
    "    count_dict = {}\n",
    "    for tag, texts in results.items():\n",
    "        classified_list = [classifier(t) for t in texts]\n",
    "        counts = [classified_list.count(t) for t in tag_list]\n",
    "        count_dict[tag] = counts + [len(classified_list) -sum(counts)]\n",
    "    df = pd.DataFrame(count_dict).T\n",
    "    df.columns = pd.MultiIndex.from_arrays([[\"Prediction\"] * (len(tag_list) + 1), tag_list + [\"others\"]])\n",
    "    df.index = pd.MultiIndex.from_arrays([[\"Truth\"] * len(tag_list), tag_list])\n",
    "    df_list.append(df)\n",
    "    acc = sum([df.iloc[i, i] for i in range(len(tag_list))]) / df.sum().sum()\n",
    "    acc_list.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b3144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b3f1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1655b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama7b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
