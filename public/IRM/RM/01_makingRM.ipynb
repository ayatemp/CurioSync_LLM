{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2af5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# モデルとトークナイザーの読み込み\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# テスト用プロンプト\n",
    "prompt = \"What is the capital of France?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力例\n",
    "output_1 = \"The capital of France is Paris.\"\n",
    "output_2 = \"France's capital is Lyon.\"\n",
    "\n",
    "# ペアワイズ比較データ\n",
    "feedback = {\"output_1\": 1, \"output_2\": 0}  # output_1が優れていると評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 報酬モデル\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.fc = nn.Linear(768, 1)  # GPT-2の隠れ層サイズに合わせる\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "reward_model = RewardModel()\n",
    "optimizer = optim.Adam(reward_model.parameters(), lr=1e-4)\n",
    "\n",
    "# 損失関数の計算と学習\n",
    "def train_reward_model(output_1, output_2, feedback):\n",
    "    reward_1 = reward_model(torch.randn(1, 768))  # ダミー入力\n",
    "    reward_2 = reward_model(torch.randn(1, 768))\n",
    "    loss = -torch.log(torch.sigmoid(reward_1 - reward_2) * feedback[\"output_1\"]\n",
    "                      + torch.sigmoid(reward_2 - reward_1) * feedback[\"output_2\"])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# 学習の実行\n",
    "for epoch in range(10):\n",
    "    loss = train_reward_model(output_1, output_2, feedback)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12482dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy():\n",
    "    reward = reward_model(torch.randn(1, 768))  # ダミー報酬\n",
    "    loss = -reward.mean()  # 報酬を最大化\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# ポリシーの更新\n",
    "for step in range(10):\n",
    "    policy_loss = update_policy()\n",
    "    print(f\"Step {step}, Policy Loss: {policy_loss}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
