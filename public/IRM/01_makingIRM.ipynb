{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch, datasets\n",
    "\n",
    "model_name = \"facebook/opt-350m\"\n",
    "tokenizer   = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# ★ポイント: num_labels=6\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=6,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config={\"load_in_4bit\": True}\n",
    ")\n",
    "\n",
    "# LoRA アダプタ\n",
    "lora_cfg = LoraConfig( task_type=\"SEQ_CLS\", r=16, lora_alpha=16, bias=\"none\")\n",
    "model = get_peft_model(base_model, lora_cfg)\n",
    "model.config.use_cache = False\n",
    "\n",
    "def preprocess(ex):\n",
    "    enc = tokenizer(ex[\"prompt\"] + ex[\"idea\"], truncation=True, max_length=512)\n",
    "    enc[\"labels\"] = [ex[f\"score_{i}\"] for i in range(6)]  # 6 次元ラベル\n",
    "    return enc\n",
    "\n",
    "ds = datasets.load_dataset(\"your_org/irm_dataset\")  # 自前データ\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./irm_logs\",\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=20,\n",
    "    save_steps=200\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"val\"]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./irm_reward_model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
